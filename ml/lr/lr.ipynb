{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression(LR)\n",
    "\n",
    "## Part I. Theory Overview\n",
    "\n",
    "### Logistic Distribution\n",
    "\n",
    "If $X$ is a continuous variable, then the CDF and PDF of logistic distribution can be represented as:\n",
    "\n",
    "$F(X) = P(X\\leq x) = \\frac{1}{1+\\exp(\\frac{-(x-\\mu)}{\\gamma})}$\n",
    "\n",
    "$f(X) = F^{'}(X) = \\frac{\\exp(\\frac{-(x-\\mu)}{\\gamma})}{\\gamma(1+\\exp(\\frac{-(x-\\mu)}{\\gamma}))^{2}}$\n",
    "\n",
    "where $F(X)$ is the sigmoid function in binomial case and $f(x)$ is symmetric about $(\\mu, \\frac{1}{2})$\n",
    "\n",
    "### Binomial Logistic Regression\n",
    "\n",
    "#### Conditional Probability\n",
    "\n",
    "Given input $X$, logistic regression predicts class $Y$ based on the conditional probability $P(Y|X)$. In the binomial case:\n",
    "\n",
    "$P(Y=1|X) = \\frac{\\exp(wx+b)}{1+\\exp(wx+b)}$\n",
    "\n",
    "$P(Y=0|X) = \\frac{1}{1+\\exp(wx+b)}$\n",
    "\n",
    "#### Odds\n",
    "\n",
    "Odds describe the ratio between probabilities. In binomial logistic regression, the log odds(logit) is:\n",
    "\n",
    "$\\hspace{5mm} log(\\frac{P}{P-1})$\n",
    "\n",
    "$=log(\\frac{P(Y=1|X)}{1-P(Y=1|X)})$\n",
    "\n",
    "$=log(\\exp(wx+b))$\n",
    "\n",
    "$=wx+b$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$P(Y=1|X) \\rightarrow 1$ as $wx+b \\rightarrow \\infty$ \n",
    "\n",
    "$P(Y=1|X) \\rightarrow 0$ as $wx+b \\rightarrow -\\infty$\n",
    "\n",
    "#### Model Parameter Estimate\n",
    "\n",
    "Let: \n",
    "\n",
    "$P(Y=1|X)=\\pi(x)$ \n",
    "\n",
    "$P(Y=0|X)=1-\\pi(x)$\n",
    "\n",
    "The likelihood function thus becomes:\n",
    "\n",
    "$l(X,Y;\\pi) = \\prod_{i=1}^{N}\\pi(x_{i})^{y_{i}}(1-\\pi(x_{i})^{1-y_{i}}$\n",
    "\n",
    "The log likelihood function is:\n",
    "\n",
    "$log(l(X,Y;\\pi)) = \\sum_{i=1}^{N}y_{i}(wx_{i})-log(1+\\exp(wx_{i}))\\hspace{1mm}$(formula simplification is omitted)\n",
    "\n",
    "where $\\pi(x)$ is assumed to be the $\\frac{\\exp(wx+b)}{1+\\exp(wx+b)}$ for logistic regression model\n",
    "\n",
    "The goal now is to find $w^{*}$ to maximize the log likelihood by gradient ascend(since we are MAXIMIZING the likelihood). The gradient of $L$ w.r.t $w$ is:\n",
    "\n",
    "$\\frac{dL}{dw} = (y_{i}-\\pi(x_{i}))x_{i}$\n",
    "\n",
    "So we update $w$ using:\n",
    "\n",
    "$w_{t+1} = w_{t} + \\alpha(y_{i}-\\pi(x_{i}))x_{i}$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "### Multinomial Logistic Regression\n",
    "\n",
    "We can easily extend the case to K-class multinomial logistic regression. Now the conditional probability function becomes:\n",
    "\n",
    "$\\frac{\\exp(w_{k}x+b)}{\\sum_{k=1}^{K}\\exp(w_{k}x+b)}, k=1,2,...,K$\n",
    "\n",
    "which is also known as the softmax function. The same optimization strategy used in binomial LR still applies to the multinomial one. Note that here the model implementation becomes more convenient if we use one-hot encoding to represent the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Logistic Regression as Multiclass Classifier\n",
    "\n",
    "In this example, we will train a logistic regression model on the Iris dataset. The model performance will be evaluated using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLR(object):\n",
    "    def __init__(self, max_iter=100, learning_rate=0.01):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, W, X):\n",
    "        return np.exp(np.dot(W, X))/np.sum(np.exp(np.dot(W, X)))\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        class_num = len(np.unique(Y))\n",
    "        X = np.append(X, np.ones((len(X),1)), 1)\n",
    "        Y_one_hot = np.zeros((len(Y), class_num))\n",
    "        Y_one_hot[np.arange(len(Y)).astype('int32'), Y.astype('int32')] = 1 # The order of class is [0, 1, -1] in the one-hot vector\n",
    "        self.W = np.zeros((class_num, X.shape[1]))\n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(len(X)):\n",
    "                dW = (Y_one_hot[i] - self.sigmoid(self.W, X[i])).reshape((-1,1)) * X[i]\n",
    "                self.W += self.learning_rate * dW\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        if X.ndim <= 1:\n",
    "            X = X.reshape((1, -1))\n",
    "        X = np.append(X, np.ones((len(X),1)), 1)\n",
    "        self.result_list = []\n",
    "        for i in range(len(X)):\n",
    "            result = np.dot(self.W, X[i])\n",
    "            Y_predict = np.argmax(result)\n",
    "            self.result_list.append(Y_predict)\n",
    "        return self.result_list\n",
    "    \n",
    "    def score(self, Y):\n",
    "        if Y.ndim <= 1:\n",
    "            Y = Y.reshape((1, -1))\n",
    "        correct_predictions = np.equal(Y, self.result_list)\n",
    "        return len(np.where(correct_predictions)[0])/Y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 1th validation, the accuracy is 96.67%\n",
      "For the 2th validation, the accuracy is 100.00%\n",
      "For the 3th validation, the accuracy is 96.67%\n",
      "For the 4th validation, the accuracy is 100.00%\n",
      "For the 5th validation, the accuracy is 96.67%\n",
      "For the 6th validation, the accuracy is 96.67%\n",
      "For the 7th validation, the accuracy is 96.67%\n",
      "For the 8th validation, the accuracy is 93.33%\n",
      "For the 9th validation, the accuracy is 100.00%\n",
      "For the 10th validation, the accuracy is 93.33%\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_iris(return_X_y=True)\n",
    "clf = MultinomialLR(max_iter=150, learning_rate=0.005)\n",
    "\n",
    "for validate_num in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    clf.train(X_train, Y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score = clf.score(Y_test)\n",
    "    print('For the {}th validation, the accuracy is {:.2f}%'.format(validate_num+1, score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 1th validation, the accuracy is 100.00%\n",
      "For the 2th validation, the accuracy is 100.00%\n",
      "For the 3th validation, the accuracy is 90.00%\n",
      "For the 4th validation, the accuracy is 93.33%\n",
      "For the 5th validation, the accuracy is 96.67%\n",
      "For the 6th validation, the accuracy is 100.00%\n",
      "For the 7th validation, the accuracy is 96.67%\n",
      "For the 8th validation, the accuracy is 96.67%\n",
      "For the 9th validation, the accuracy is 100.00%\n",
      "For the 10th validation, the accuracy is 96.67%\n"
     ]
    }
   ],
   "source": [
    "sk_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "for validate_num in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    sk_clf.fit(X_train, Y_train)\n",
    "    score = sk_clf.score(X_test, Y_test)\n",
    "    print('For the {}th validation, the accuracy is {:.2f}%'.format(validate_num+1, score*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
